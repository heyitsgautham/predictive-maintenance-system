# ğŸ”§ Predictive Maintenance using PySpark

![PySpark](https://img.shields.io/badge/PySpark-2.0.2-orange?logo=apache-spark)
![Python](https://img.shields.io/badge/Python-2.7.5-blue?logo=python)
![ML](https://img.shields.io/badge/Machine%20Learning-Classification-green)
![Status](https://img.shields.io/badge/Status-Production%20Ready-success)

> Built an end-to-end machine learning pipeline to predict equipment failures 7 days in advance using distributed computing with PySpark. Processed 2M+ records across 1,900 machines to achieve high-precision failure prediction.

## ğŸ“Š Project Stats

| Metric | Value |
|--------|-------|
| ğŸ“¦ **Dataset Size** | 1.3 GB / ~2M records |
| ğŸ­ **Machines Monitored** | 1,900 devices |
| â±ï¸ **Time Period** | 4 years |
| ğŸ“ˆ **Features (Raw)** | 172 columns |
| ğŸ”„ **Rolling Features** | 1,150 engineered |
| ğŸ¯ **Final Features** | 140 (post-PCA) |
| ğŸšï¸ **Prediction Window** | 7 days ahead |
| ğŸ“‰ **Class Imbalance** | 98.5% / 1.5% |
| ğŸ¯ **Model** | Random Forest (optimized) |

## ğŸš€ Key Technical Achievements

### ğŸ”¨ Data Engineering
- âœ… Resolved Spark CSV parsing issues with null handling
- âœ… Implemented comprehensive data quality pipeline
- âœ… Reduced dimensionality using PCA (172 â†’ 50 components)
- âœ… Created 1,150 rolling statistics features (5 windows Ã— 46 features Ã— 5 stats)

### âš¡ Performance Optimization
- âœ… Overcame Spark StackOverflow errors via workload chunking
- âœ… Optimized join operations using `coalesce()` vs `repartition()`
- âœ… Leveraged Parquet format for schema preservation
- âœ… Utilized 32-core VM with full CPU utilization

### ğŸ¤– Machine Learning
- âœ… Implemented over-labeling technique for early warning (7-day window)
- âœ… Applied stratified down-sampling (1:10 ratio) for class balance
- âœ… Built Random Forest & Gradient-Boosted Tree classifiers
- âœ… Performed grid search with 3-fold cross-validation
- âœ… Optimized for precision over recall (business cost-driven)

## ğŸ““ Notebook Structure

| Notebook | Focus Area | Key Outputs |
|----------|-----------|-------------|
| **01_DataCleansing_FeatureEngineering** | Data cleaning, EDA, feature creation | 172 â†’ 140 features |
| **02_FeatureEngineering_RollingCompute** | Time-series rolling statistics | +1,150 temporal features |
| **03_Labeling_FeatureSelection_Modeling** | ML pipeline, model training & tuning | Production-ready model |

## ğŸ› ï¸ Tech Stack

```
Big Data:      Apache Spark 2.0.2, PySpark
ML/DL:         Spark MLlib, scikit-learn
Processing:    32-core Linux DSVM (448 GB RAM)
Storage:       Azure Blob Storage, Parquet
Visualization: Matplotlib, Seaborn
```

## ğŸ¯ Business Impact

- ğŸ” **Proactive Maintenance**: 7-day advance warning enables resource planning
- ğŸ’° **Cost Optimization**: High precision minimizes false positive costs
- âš™ï¸ **Scalable**: Architecture supports multi-TB datasets
- ğŸ“¦ **Production Ready**: Saved optimized model for deployment

## ğŸ“ˆ Model Performance

- **Algorithm**: Random Forest (100 trees, hyperparameter tuned)
- **Optimization Metric**: Weighted Precision
- **Evaluation**: ROC-AUC, Precision-Recall, F1-Score
- **Validation**: 3-fold cross-validation with temporal split

---

â­ **Built with scalability and production deployment in mind**